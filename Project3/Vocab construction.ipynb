{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fdee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vars(a, axis=None):\n",
    "    \"\"\" Variance of sparse matrix a\n",
    "    var = mean(a**2) - mean(a)**2\n",
    "    \"\"\"\n",
    "    a_squared = a.copy()\n",
    "    a_squared.data **= 2\n",
    "    return a_squared.mean(axis) - np.square(a.mean(axis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb04e2",
   "metadata": {},
   "source": [
    "# Load the whole data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdddd17",
   "metadata": {},
   "source": [
    "I construct my vocabulary using the whole data `alldata.tsv`. Html tags are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.read_csv('./alldata.tsv', sep='\\t', encoding='utf-8')\n",
    "cleaner = re.compile('<.*?>')\n",
    "alldata['review'] = alldata['review'].map(lambda s: re.sub(cleaner, '', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4f46c",
   "metadata": {},
   "source": [
    "# Construct DT matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa2ae5",
   "metadata": {},
   "source": [
    "I use `sklearn.feature_extraction.text.CountVectorizer()` to get the matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a329905",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"i\", \"me\", \"my\", \"myself\", \n",
    "               \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "               \"you\", \"your\", \"yours\", \n",
    "               \"their\", \"they\", \"his\", \"her\", \n",
    "               \"she\", \"he\", \"a\", \"an\", \"and\",\n",
    "               \"is\", \"was\", \"are\", \"were\", \n",
    "               \"him\", \"himself\", \"has\", \"have\", \n",
    "               \"it\", \"its\", \"the\", \"us\"]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stop_words, ngram_range=(1, 8), min_df=0.001, max_df=0.5)\n",
    "corpus = alldata['review'].values\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f0f09",
   "metadata": {},
   "source": [
    "# Pick top 2500 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0c053",
   "metadata": {},
   "source": [
    "Follow Prof. Liang's suggestion. I calculate t-statistics of every word. Then I pick top 2500 words with largest absolute t-statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df80644",
   "metadata": {},
   "outputs": [],
   "source": [
    "indi = (alldata['sentiment'] == 1)\n",
    "size = X.shape[1]\n",
    "summ = np.zeros((size, 4))\n",
    "# mean_1\n",
    "summ[:,0] = np.array(X[indi].mean(axis=0)).reshape(-1)\n",
    "# var_1 \n",
    "summ[:,1] = np.array(vars(X[indi], axis=0)).reshape(-1)\n",
    "# mean_2\n",
    "summ[:,2] = np.array(X[~indi].mean(axis=0)).reshape(-1)\n",
    "# var_2\n",
    "summ[:,3] = np.array(vars(X[~indi], axis=0)).reshape(-1)\n",
    "\n",
    "n1 = sum(alldata['sentiment'])\n",
    "n2 = len(alldata) - n1\n",
    "tstat = (summ[:,0] - summ[:,2]) / np.sqrt(summ[:,1] / n1 + summ[:,3] / n2)\n",
    "\n",
    "idx = np.argsort(-np.abs(tstat))[:2500]\n",
    "pos_idx = idx[tstat[idx] >= 0]\n",
    "neg_idx = idx[tstat[idx] < 0]\n",
    "words = np.array(vectorizer.get_feature_names(), dtype='str')\n",
    "pos_words = words[pos_idx]\n",
    "neg_words = words[neg_idx]\n",
    "vocab_2500 = pos_words.tolist() + neg_words.tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478aaac4",
   "metadata": {},
   "source": [
    "# Reduce the vocab size to 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9283abd",
   "metadata": {},
   "source": [
    "I use logistic regression with l1 penalty `C=0.0955` to reduce the vocab size to 999.  \n",
    "Then I save my vocabulary to `myvocab.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "voca = dict()\n",
    "for idx, term in enumerate(vocab_2500):\n",
    "    voca[term] = idx\n",
    "vectorizer = CountVectorizer(vocabulary=voca, ngram_range=(1, 10))\n",
    "\n",
    "X = vectorizer.transform(alldata['review'].values).toarray()\n",
    "X_train = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "Y_train = alldata['sentiment']\n",
    "\n",
    "lasso = LogisticRegression(penalty='l1', C=0.0955, solver='liblinear', random_state=2021)\n",
    "lasso.fit(X_train, Y_train)\n",
    "coef = lasso.coef_.reshape(-1)\n",
    "features = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "with open('./myvocab.txt', mode='w', encoding='UTF-8') as f:\n",
    "    for w in features[coef != 0].tolist():\n",
    "        f.write(w + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
